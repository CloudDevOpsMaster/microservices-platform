# ResoluciÃ³n de Problema: Falla en Sistema de Microservicios

## ðŸ“‹ DESCRIPCIÃ“N DEL INCIDENTE

**Reporte inicial:**
- âŒ Usuarios no pueden guardar registros
- âŒ Errores 500 en algunos microservicios
- âŒ Alta latencia en respuestas de agentes IA

**Severidad:** CRÃTICA (P1)
**Impacto:** Funcionalidad principal comprometida

---

## ðŸ” FASE 1: HIPÃ“TESIS INICIALES (Primeros 5 min)

### HipÃ³tesis priorizadas por probabilidad:

**1. Base de datos (ALTA probabilidad)**
- PostgreSQL saturado por conexiones no cerradas
- MongoDB sin Ã­ndices en colecciones de auditorÃ­a
- Redis con memoria llena (eviction de cachÃ©)

**2. ComunicaciÃ³n entre servicios (ALTA probabilidad)**
- RabbitMQ con colas saturadas (eventos no procesados)
- Timeouts en llamadas REST entre servicios
- Circuit breakers abiertos por fallos en cascada

**3. Agentes IA (MEDIA-ALTA probabilidad)**
- Rate limiting de APIs (OpenAI/Bedrock)
- Context window excedido (tokens > lÃ­mite)
- Embeddings pipeline bloqueado
- Vector DB (Qdrant) con consultas lentas

**4. Problemas de red/infraestructura (MEDIA probabilidad)**
- Contenedores Docker sin recursos (CPU/memoria)
- Puertos bloqueados o servicios caÃ­dos
- DNS interno de Docker no resuelve nombres

---

## ðŸ› ï¸ FASE 2: PLAN DE DIAGNÃ“STICO SISTEMÃTICO (10 min)

### Paso 1: Verificar estado de servicios (30 seg)
```bash
# Check all containers
docker-compose ps

# Quick health check
curl http://localhost:8001/health  # Auth
curl http://localhost:8002/health  # User
curl http://localhost:8003/health  # Audit
curl http://localhost:8004/health  # AI
```

**AcciÃ³n:** Identificar quÃ© servicios estÃ¡n caÃ­dos o degradados.

---

### Paso 2: Analizar logs estructurados (3 min)

**Prioridad de revisiÃ³n:**
1. User Service (donde ocurre el error de guardado)
2. AI Service (alta latencia reportada)
3. RabbitMQ (comunicaciÃ³n asÃ­ncrona)

```bash
# Logs en tiempo real con filtro de errores
docker-compose logs -f --tail=100 user-service | grep -E "ERROR|500"
docker-compose logs -f --tail=100 ai-service | grep -E "latency|timeout"
docker-compose logs -f rabbitmq | grep -E "queue|overflow"
```

**Buscar en logs JSON:**
- `request_id` para rastrear flujo completo
- `error_type` y `stack_trace`
- `response_time_ms` para identificar cuellos de botella
- `db_query_duration` para problemas de DB

---

### Paso 3: Verificar bases de datos (2 min)

```bash
# PostgreSQL - conexiones activas
docker exec -it postgres psql -U admin -d users_db -c \
  "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"

# PostgreSQL - queries lentas
docker exec -it postgres psql -U admin -d users_db -c \
  "SELECT query, state, wait_event_type FROM pg_stat_activity WHERE state != 'idle';"

# MongoDB - operaciones en curso
docker exec -it mongodb mongosh --eval "db.currentOp()"

# Redis - uso de memoria
docker exec -it redis redis-cli INFO memory | grep used_memory_human
```

**Indicadores crÃ­ticos:**
- PostgreSQL: >80% conexiones mÃ¡ximas
- MongoDB: operaciones bloqueadas (locks)
- Redis: memoria >90% del lÃ­mite

---

### Paso 4: Verificar RabbitMQ (1 min)

```bash
# Management UI
# http://localhost:15672 (admin/admin123)

# O via CLI
docker exec -it rabbitmq rabbitmqctl list_queues name messages consumers
```

**SeÃ±ales de alerta:**
- Colas con >1000 mensajes sin procesar
- Consumidores en 0 (Audit Service caÃ­do)
- Rate de mensajes entrantes > rate de procesamiento

---

### Paso 5: DiagnÃ³stico especÃ­fico de IA (2 min)

```bash
# Logs de AI Service - buscar rate limiting
docker-compose logs ai-service | grep -E "rate_limit|429|quota"

# Verificar latencia de llamadas a Bedrock/OpenAI
docker-compose logs ai-service | grep "llm_call_duration"

# Estado de Qdrant
curl http://localhost:6333/collections
```

**Problemas comunes:**
- HTTP 429: Rate limit excedido
- HTTP 503: Servicio IA temporalmente no disponible
- Latencia >5s: Context window muy grande o embeddings lentos
- Qdrant: Colecciones sin Ã­ndices optimizados

---

### Paso 6: Verificar recursos de contenedores (1 min)

```bash
# Uso de CPU/memoria por contenedor
docker stats --no-stream

# Eventos de OOM (Out of Memory)
docker inspect user-service | grep -i oom
```

---

## ðŸŽ¯ FASE 3: DIAGNÃ“STICO PROBABLE (Basado en sÃ­ntomas)

### Escenario A: Error 500 en User Service al guardar

**DiagnÃ³stico mÃ¡s probable:**
1. PostgreSQL con conexiones saturadas
2. SQLAlchemy sin `pool_pre_ping=True` (conexiones muertas)
3. Transacciones no commiteadas bloqueando escrituras

**SoluciÃ³n inmediata:**
```bash
# Reiniciar pool de conexiones
docker-compose restart user-service

# Si persiste, reiniciar PostgreSQL (Ãºltimo recurso)
docker-compose restart postgres
```

**Fix permanente:**
```python
# user-service/app/infrastructure/database.py
engine = create_engine(
    DATABASE_URL,
    pool_size=10,
    max_overflow=5,
    pool_pre_ping=True,  # â† Detecta conexiones muertas
    pool_recycle=3600    # â† Recicla cada hora
)
```

---

### Escenario B: Alta latencia en AI Service

**DiagnÃ³stico mÃ¡s probable:**
1. Rate limiting de API (429 errors)
2. Context window excedido (>200K tokens)
3. Qdrant sin Ã­ndices HNSW optimizados

**SoluciÃ³n inmediata:**
```python
# ai-service/app/infrastructure/groq/groq_client.py
# Agregar retry con exponential backoff
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(RateLimitError)
)
async def generate(self, prompt, context):
    # Limitar context window
    context_tokens = self.count_tokens(context)
    if context_tokens > 100000:  # 50% del lÃ­mite
        context = self.truncate_context(context, max_tokens=100000)
    
    response = await self.client.generate(...)
```

**Fix Qdrant:**
```python
# Optimizar bÃºsqueda vectorial
client.update_collection(
    collection_name="documents",
    hnsw_config=models.HnswConfigDiff(
        m=16,  # Reduce latencia
        ef_construct=100
    )
)
```

---

### Escenario C: RabbitMQ saturado

**DiagnÃ³stico:**
- Audit Service caÃ­do (consumer no procesa)
- Colas con backlog masivo

**SoluciÃ³n:**
```bash
# Reiniciar consumer
docker-compose restart audit-service

# Monitorear procesamiento
watch -n 1 'docker exec rabbitmq rabbitmqctl list_queues'
```

---

## ðŸ“Š FASE 4: MÃ‰TRICAS Y MONITOREO

### Logs centralizados a revisar:

```json
// Ejemplo de log estructurado para seguir el flujo
{
  "timestamp": "2025-12-15T10:30:00Z",
  "level": "ERROR",
  "service": "user-service",
  "request_id": "abc-123",
  "user_id": "user-456",
  "endpoint": "/users",
  "method": "POST",
  "status_code": 500,
  "error_type": "DatabaseConnectionError",
  "error_message": "FATAL: remaining connection slots reserved",
  "db_query_duration_ms": 5000,
  "response_time_ms": 5100
}
```

**Usar request_id para rastreo:**
```bash
# Seguir flujo completo de una request fallida
docker-compose logs | grep "abc-123"
```

---

## ðŸ“¢ FASE 5: COMUNICACIÃ“N A STAKEHOLDERS

### T+5 min (Update inicial)
**Slack/Email:**
```
ðŸ”´ INCIDENTE CRÃTICO - Sistema de usuarios
Estado: Investigando
Impacto: Usuarios no pueden guardar registros
Equipo: Revisando logs de DB y servicios
ETA prÃ³ximo update: 10 minutos
```

### T+15 min (DiagnÃ³stico)
```
ðŸŸ¡ INCIDENTE EN PROGRESO
Causa identificada: Pool de conexiones PostgreSQL saturado
AcciÃ³n: Aplicando fix de configuraciÃ³n + reinicio controlado
Impacto: Sistema temporalmente en modo solo-lectura
ETA resoluciÃ³n: 15 minutos
```

### T+30 min (ResoluciÃ³n)
```
ðŸŸ¢ INCIDENTE RESUELTO
Causa raÃ­z: Pool de conexiones sin reciclaje automÃ¡tico
SoluciÃ³n aplicada: ConfiguraciÃ³n pool_pre_ping + pool_recycle
Estado: Sistema operando normalmente
Monitoreo: Continuamos observando mÃ©tricas por 2 horas
Post-mortem: Programado para maÃ±ana 10am
```

---

## ðŸ”§ ACCIONES PREVENTIVAS

### Corto plazo (esta semana):
1. **Agregar health checks robustos:**
```python
@router.get("/health")
async def health_check(db: Session):
    checks = {
        "database": check_db_connection(db),
        "redis": check_redis_connection(),
        "rabbitmq": check_rabbitmq_connection()
    }
    if not all(checks.values()):
        raise HTTPException(503, detail=checks)
    return {"status": "healthy", "checks": checks}
```

2. **Implementar circuit breakers:**
```python
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
async def call_ai_service(query):
    # Si falla 5 veces, abre circuito por 60s
    return await ai_client.query(query)
```

3. **Agregar alertas proactivas:**
- CPU >80% por 5 min
- Memoria >85%
- Colas RabbitMQ >500 mensajes
- Latencia P95 >2s

### Mediano plazo (prÃ³ximo sprint):
1. Implementar APM (Application Performance Monitoring)
2. Logs centralizados en ELK/Loki
3. Dashboards de mÃ©tricas en Grafana
4. Tests de carga automatizados

---

## âœ… CHECKLIST DE RESOLUCIÃ“N

- [ ] Identificar servicio(s) afectado(s)
- [ ] Revisar logs estructurados con request_id
- [ ] Verificar estado de DBs (conexiones, queries)
- [ ] Verificar RabbitMQ (colas, consumidores)
- [ ] Diagnosticar problemas de IA (rate limits, latencia)
- [ ] Aplicar fix temporal (restart si es necesario)
- [ ] Validar resoluciÃ³n con tests manuales
- [ ] Comunicar resoluciÃ³n a stakeholders
- [ ] Aplicar fix permanente en cÃ³digo
- [ ] Deploy de fix con tests
- [ ] Monitoreo post-incidente (2 horas)
- [ ] Documentar en post-mortem

---

## ðŸ“ˆ MÃ‰TRICAS POST-INCIDENTE

**Monitorear por 2 horas:**
- Tasa de errores 500: <0.1%
- Latencia P95: <500ms
- Latencia AI Service: <3s
- Uso de conexiones DB: <60%
- Backlog RabbitMQ: <50 mensajes

**SLA objetivo:**
- Tiempo de detecciÃ³n: <5 min
- Tiempo de diagnÃ³stico: <15 min
- Tiempo de resoluciÃ³n: <30 min
- MTTR (Mean Time To Recovery): <1 hora